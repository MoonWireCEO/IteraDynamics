name: Walkforward CV

permissions:
  contents: read
  actions: read

on:
  workflow_dispatch:
    inputs:
      symbols:
        description: "Comma-separated symbols to evaluate"
        required: false
        default: "SPY,QQQ"
      k_folds:
        description: "Number of contiguous folds"
        required: false
        default: "5"
      window_hours:
        description: "Evaluation window in hours (fallback if no backfill is pulled)"
        required: false
        default: "720"
      backfill_run_id:
        description: "Specific run id of backfill-ml-shadow to pull (optional)"
        required: false
        default: ""
      conf_min_override:
        description: "Override conf_min during gating (blank = use governance/backfill row)"
        required: false
        default: ""
      deadband_override:
        description: "Override deadband for paper trading (blank = use config file default: 0.08)"
        required: false
        default: ""
      min_flip_min_override:
        description: "Override min_flip_min for paper trading in minutes (blank = use config file default: 360)"
        required: false
        default: ""
      lookback_h_override:
        description: "Override lookback_h for paper trading replay window (blank = use config file default: 720)"
        required: false
        default: ""
      position_size_pct:
        description: "Position size as percentage of capital per trade (blank = use config file default: 1.0 = 100%)"
        required: false
        default: ""
      stop_loss_pct:
        description: "Stop loss percentage (blank = use config file default: null = no stop loss)"
        required: false
        default: ""

jobs:
  walkforward:
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}
      MPLBACKEND: Agg
      SYMS: ${{ inputs.symbols }}
      KFOLDS: ${{ inputs.k_folds }}
      WIN_H: ${{ inputs.window_hours }}
      CONF_MIN_OVERRIDE: ${{ inputs.conf_min_override }}
      DEADBAND_OVERRIDE: ${{ inputs.deadband_override }}
      MIN_FLIP_MIN_OVERRIDE: ${{ inputs.min_flip_min_override }}
      LOOKBACK_H_OVERRIDE: ${{ inputs.lookback_h_override }}
      POSITION_SIZE_PCT: ${{ inputs.position_size_pct }}
      STOP_LOSS_PCT: ${{ inputs.stop_loss_pct }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then python -m pip install -r requirements-dev.txt; fi
          python -m pip install matplotlib

      # â”€â”€ Pull the BACKFILL shadow log artifact into logs/incoming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Download backfilled shadow log (latest success)
        if: ${{ inputs.backfill_run_id == '' }}
        uses: dawidd6/action-download-artifact@v3
        with:
          repository: ${{ github.repository }}
          workflow: backfill-ml-shadow
          workflow_conclusion: success
          name: ml-shadow-backfill
          path: logs/incoming
          allow_forks: true
          if_no_artifact_found: warn

      - name: Download backfilled shadow log by run id (optional)
        if: ${{ inputs.backfill_run_id != '' }}
        uses: dawidd6/action-download-artifact@v3
        with:
          run_id: ${{ inputs.backfill_run_id }}
          repo: ${{ github.repository }}
          name: ml-shadow-backfill
          path: logs/incoming
          if_no_artifact_found: warn

      - name: Verify/choose shadow file (incoming or fallback)
        id: choose_shadow
        run: |
          set -euo pipefail
          mkdir -p logs
          # Prefer downloaded backfill; else fall back to existing canonical file if present
          if [ -f logs/incoming/signal_inference_shadow.jsonl ]; then
            cp -v logs/incoming/signal_inference_shadow.jsonl logs/signal_inference_shadow_backfill.jsonl
            echo "shadow_path=logs/signal_inference_shadow_backfill.jsonl" >> "$GITHUB_OUTPUT"
          elif [ -f logs/signal_inference_shadow.jsonl ]; then
            echo "Using existing logs/signal_inference_shadow.jsonl as source"
            echo "shadow_path=logs/signal_inference_shadow.jsonl" >> "$GITHUB_OUTPUT"
          else
            # last resort: create an empty file so later steps can handle gracefully
            : > logs/signal_inference_shadow_backfill.jsonl
            echo "shadow_path=logs/signal_inference_shadow_backfill.jsonl" >> "$GITHUB_OUTPUT"
          fi
          echo "Chosen shadow source: $(cat $GITHUB_OUTPUT)"

      - name: Walkforward CV (slice backfill into K folds and replay per fold)
        env:
          SHADOW_SRC: ${{ steps.choose_shadow.outputs.shadow_path }}
        run: |
          python - <<'PY'
          import os, json, math, subprocess, sys
          from pathlib import Path
          from datetime import datetime, timedelta, timezone

          syms = [s.strip().upper() for s in os.getenv("SYMS","SPY,QQQ").split(",") if s.strip()]
          k = max(2, int(os.getenv("KFOLDS","5")))
          win_h = int(os.getenv("WIN_H","720"))
          conf_min_override = os.getenv("CONF_MIN_OVERRIDE","").strip()

          # Paper trading override parameters (passed to replay_shadow_to_paper)
          deadband_override = os.getenv("DEADBAND_OVERRIDE","").strip()
          min_flip_min_override = os.getenv("MIN_FLIP_MIN_OVERRIDE","").strip()
          lookback_h_override = os.getenv("LOOKBACK_H_OVERRIDE","").strip()
          position_size_pct = os.getenv("POSITION_SIZE_PCT","").strip()
          stop_loss_pct = os.getenv("STOP_LOSS_PCT","").strip()

          src = Path(os.getenv("SHADOW_SRC","logs/signal_inference_shadow_backfill.jsonl"))
          dst = Path("logs") / "signal_inference_shadow.jsonl"  # canonical that paper_trader reads
          metrics_path = Path("models") / "performance_metrics.json"

          def parse_ts(s):
              s = str(s)
              if s.endswith("Z"): s = s[:-1] + "+00:00"
              try:
                  return datetime.fromisoformat(s).astimezone(timezone.utc)
              except Exception:
                  return None

          # Load source rows (backfill)
          rows = []
          if src.exists():
              for ln in src.read_text(encoding="utf-8").splitlines():
                  try:
                      j = json.loads(ln)
                      if not j.get("symbol"): continue
                      if j["symbol"].upper() not in syms: continue
                      t = parse_ts(j.get("ts"))
                      if not t: continue
                      j["_ts"] = t  # helper for sorting/windowing ONLY
                      rows.append(j)
                  except Exception:
                      pass

          if not rows:
              print(f"[WF] No rows available in {src}. Nothing to evaluate.")
              print(json.dumps({"folds": 0, "by_symbol": {}, "aggregate": {}}, indent=2))
              raise SystemExit(0)

          rows.sort(key=lambda r: r["_ts"])
          end = rows[-1]["_ts"]
          start = end - timedelta(hours=win_h)
          # Filter to window
          rows = [r for r in rows if start <= r["_ts"] <= end]
          if not rows:
              print(f"[WF] No rows within window {start} .. {end}")
              raise SystemExit(0)

          # Build K contiguous folds by time
          span = (rows[-1]["_ts"] - rows[0]["_ts"]).total_seconds()
          fold_sec = span / k
          fold_edges = [rows[0]["_ts"] + timedelta(seconds=i*fold_sec) for i in range(k)] + [rows[-1]["_ts"] + timedelta(seconds=1)]

          print(f"[WF] symbols={syms}  k={k}  rows_in_win={len(rows)}  window={rows[0]['_ts']} .. {rows[-1]['_ts']}")
          summary = {"folds": [], "aggregate": None}

          def conf_min_for_row(j):
              if conf_min_override:
                  try: return float(conf_min_override)
                  except: return 0.6
              gov = j.get("gov") or {}
              try: return float(gov.get("conf_min", 0.6))
              except: return 0.6

          all_agg = {"trades":0,"by_symbol":{}}
          fold_metrics_list = []  # Store metrics from each fold for aggregation

          for fi in range(k):
              f_start = fold_edges[fi]
              f_end   = fold_edges[fi+1]
              fold_rows = []
              for j in rows:
                  t = j["_ts"]
                  if not (f_start <= t < f_end): continue
                  if not j.get("ml_ok", False): continue
                  conf = float(j.get("ml_conf") or 0.0)
                  if conf < conf_min_for_row(j): continue
                  # copy and strip helper key to keep JSON serializable
                  jj = dict(j)
                  jj.pop("_ts", None)
                  fold_rows.append(jj)

              # Write this fold to canonical shadow and run paper trader replay
              dst.parent.mkdir(parents=True, exist_ok=True)
              with dst.open("w", encoding="utf-8") as w:
                  for jj in fold_rows:
                      w.write(json.dumps(jj) + "\n")

              print(f"[WF] Fold {fi+1}/{k}: rows={len(fold_rows)}  range={f_start}..{f_end}")

              # Build env for replay_shadow_to_paper with overrides
              replay_env = os.environ.copy()
              if deadband_override:
                  replay_env["AE_PERF_DEADBAND"] = deadband_override
              if min_flip_min_override:
                  replay_env["AE_PERF_MIN_FLIP_MIN"] = min_flip_min_override
              if lookback_h_override:
                  replay_env["AE_PERF_LOOKBACK_H"] = lookback_h_override
              if position_size_pct:
                  replay_env["AE_PERF_POSITION_SIZE_PCT"] = position_size_pct
              if stop_loss_pct:
                  replay_env["AE_PERF_STOP_LOSS_PCT"] = stop_loss_pct

              subprocess.run([sys.executable, "-m", "scripts.perf.replay_shadow_to_paper"], check=True, env=replay_env)

              # Read per-fold metrics
              if metrics_path.exists():
                  met = json.loads(metrics_path.read_text(encoding="utf-8"))
              else:
                  met = {"aggregate":{"trades":0},"by_symbol":{}}

              met["fold"] = fi+1
              met["range"] = {"start": f_start.isoformat(), "end": f_end.isoformat()}
              summary["folds"].append(met)

              # Store for aggregation
              fold_metrics_list.append(met)

              # accumulate rough aggregates
              all_agg["trades"] += int(met.get("aggregate",{}).get("trades",0))
              for s, sv in (met.get("by_symbol") or {}).items():
                  d = all_agg["by_symbol"].setdefault(s, {"trades":0})
                  d["trades"] += int(sv.get("trades",0))

          # Calculate comprehensive aggregate metrics across all folds
          if fold_metrics_list:
              def safe_float(x, default=0.0):
                  try: return float(x) if x is not None else default
                  except: return default

              # Aggregate overall metrics
              total_trades = sum(safe_float(m.get("aggregate",{}).get("trades",0)) for m in fold_metrics_list)

              # Average metrics
              sharpe_vals = [safe_float(m.get("aggregate",{}).get("sharpe")) for m in fold_metrics_list if m.get("aggregate",{}).get("sharpe") is not None]
              sortino_vals = [safe_float(m.get("aggregate",{}).get("sortino")) for m in fold_metrics_list if m.get("aggregate",{}).get("sortino") is not None]
              cagr_vals = [safe_float(m.get("aggregate",{}).get("cagr")) for m in fold_metrics_list if m.get("aggregate",{}).get("cagr") is not None]
              signals_per_day_vals = [safe_float(m.get("aggregate",{}).get("signals_per_day")) for m in fold_metrics_list if m.get("aggregate",{}).get("signals_per_day") is not None]

              # Weighted averages (by trade count)
              win_rate_num = sum(safe_float(m.get("aggregate",{}).get("win_rate",0)) * safe_float(m.get("aggregate",{}).get("trades",0)) for m in fold_metrics_list)
              profit_factor_num = sum(safe_float(m.get("aggregate",{}).get("profit_factor",0)) * safe_float(m.get("aggregate",{}).get("trades",0)) for m in fold_metrics_list)

              # Max drawdown - take worst (most negative)
              dd_vals = [safe_float(m.get("aggregate",{}).get("max_drawdown")) for m in fold_metrics_list if m.get("aggregate",{}).get("max_drawdown") is not None]

              all_agg.update({
                  "trades": int(total_trades),
                  "signals_per_day": round(sum(signals_per_day_vals) / len(signals_per_day_vals), 2) if signals_per_day_vals else None,
                  "sharpe": round(sum(sharpe_vals) / len(sharpe_vals), 3) if sharpe_vals else None,
                  "sortino": round(sum(sortino_vals) / len(sortino_vals), 3) if sortino_vals else None,
                  "max_drawdown": round(min(dd_vals), 4) if dd_vals else None,
                  "win_rate": round(win_rate_num / total_trades, 3) if total_trades > 0 else None,
                  "profit_factor": round(profit_factor_num / total_trades, 3) if total_trades > 0 else None,
                  "cagr": round(sum(cagr_vals) / len(cagr_vals), 4) if cagr_vals else None,
              })

          summary["aggregate"] = all_agg
          Path("models/walkforward_summary.json").write_text(json.dumps(summary, indent=2, default=str), encoding="utf-8")
          print("[WF] Summary:")
          print(json.dumps(summary, indent=2, default=str))
          PY


      - name: Append summary to job summary
        run: |
          echo "## ðŸ§ª Walkforward CV (Backfill-based) Results" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "**Symbols:** \`${{ inputs.symbols }}\`  â€¢  **K:** \`${{ inputs.k_folds }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "**Backfill source:** \`${{ inputs.backfill_run_id != '' && inputs.backfill_run_id || 'latest success' }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "**Fold metrics (per fold):**" >> "$GITHUB_STEP_SUMMARY"
          echo "\`\`\`json" >> "$GITHUB_STEP_SUMMARY"
          test -f models/walkforward_summary.json && cat models/walkforward_summary.json >> "$GITHUB_STEP_SUMMARY" || echo "{ }" >> "$GITHUB_STEP_SUMMARY"
          echo "\`\`\`" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload WF artifacts
        uses: actions/upload-artifact@v4
        with:
          name: walkforward-cv-artifacts
          if-no-files-found: warn
          path: |
            logs/signal_inference_shadow.jsonl
            logs/paper_trades.jsonl
            models/performance_metrics.json
            models/walkforward_summary.json
